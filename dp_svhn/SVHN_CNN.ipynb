{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLFFk6AXijQj",
        "outputId": "3f21b3a1-f1c9-40bf-c63b-dc0f98cda6b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  173M  100  173M    0     0  5696k      0  0:00:31  0:00:31 --:--:-- 2093k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 61.2M  100 61.2M    0     0  12.8M      0  0:00:04  0:00:04 --:--:-- 13.0M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  173M  100  173M    0     0  5441k      0  0:00:32  0:00:32 --:--:-- 9352k\n"
          ]
        }
      ],
      "source": [
        "# Baixar os arquivos\n",
        "!curl -O http://ufldl.stanford.edu/housenumbers/train_32x32.mat\n",
        "!curl -O http://ufldl.stanford.edu/housenumbers/test_32x32.mat\n",
        "!curl -O http://ufldl.stanford.edu/housenumbers/train_32x32.mat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "FPXkbkj2iFJg"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "import scipy.io\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import InputLayer, Dense, Flatten, Dropout, MaxPooling2D, BatchNormalization, Conv2D\n",
        "from tensorflow.keras import utils as np_utils\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "EBDI8Mh5lt0s"
      },
      "outputs": [],
      "source": [
        "train_file = 'train_32x32.mat'\n",
        "test_file = 'test_32x32.mat'\n",
        "\n",
        "# Carregando os arquivos Matlab\n",
        "train_data = scipy.io.loadmat(train_file) # ['__header__', '__version__', '__globals__', 'X', 'y'] - esturutra\n",
        "test_data = scipy.io.loadmat(test_file) # ['__header__', '__version__', '__globals__', 'X', 'y'] - esturutra\n",
        "\n",
        "# Coletando os X e y train e test\n",
        "train_images = train_data['X']\n",
        "train_labels = train_data['y']\n",
        "\n",
        "test_images = test_data['X']\n",
        "test_labels = test_data['y']\n",
        "\n",
        "# Transposição da ordem (num_image, altura, largura, canais)\n",
        "train_images = np.transpose(train_images, (3, 0, 1, 2))\n",
        "test_images = np.transpose(test_images, (3, 0, 1, 2))\n",
        "\n",
        "# Colunas para linhas nas labels\n",
        "train_labels = train_labels.flatten()\n",
        "test_labels = test_labels.flatten()\n",
        "\n",
        "# Ajuste as labels para a base 0\n",
        "train_labels = train_labels.flatten() - 1\n",
        "test_labels = test_labels.flatten() - 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "fMjzqmKzx8No"
      },
      "outputs": [],
      "source": [
        "#Convertendo as bases y de treinamento e teste para categorico\n",
        "y_treinamento = to_categorical(train_labels, 10)\n",
        "y_teste = to_categorical(test_labels, 10)\n",
        "\n",
        "#RGB > Escalas de cinzas\n",
        "train_images_rgb = train_images.astype('float32') / 255.0\n",
        "test_images_rgb = test_images.astype('float32') / 255.0\n",
        "\n",
        "train_images_gray = tf.image.rgb_to_grayscale(train_images_rgb).numpy()\n",
        "test_images_gray = tf.image.rgb_to_grayscale(test_images_rgb).numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcK0eukGig51"
      },
      "source": [
        "**Importação das bases**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DC4tUFWdjCRY"
      },
      "source": [
        "**Arquitetura do Modelo**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "xm_V41IYjAAV"
      },
      "outputs": [],
      "source": [
        "rede_neural = Sequential()\n",
        "\n",
        "rede_neural.add(InputLayer(shape =(32, 32, 1)))\n",
        "\n",
        "rede_neural.add(Conv2D(filters = 32, kernel_size = (3, 3), activation = 'relu'))\n",
        "rede_neural.add(BatchNormalization())\n",
        "rede_neural.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "\n",
        "rede_neural.add(Flatten())\n",
        "\n",
        "rede_neural.add(Dense(units = 128, activation = 'relu'))\n",
        "rede_neural.add(Dropout(0.2))\n",
        "rede_neural.add(Dense(units = 128, activation = 'relu'))\n",
        "rede_neural.add(Dropout(0.2))\n",
        "rede_neural.add(Dense(units = 10, activation = 'softmax'))\n",
        "rede_neural.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9b2uWwJ6U5D"
      },
      "source": [
        "**Treinamento do Modelo**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWbZPT4Aq9kP",
        "outputId": "360eb18f-e392-49f1-a3ac-9b105a40d28c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m573/573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 145ms/step - accuracy: 0.5515 - loss: 1.3753 - val_accuracy: 0.7686 - val_loss: 0.9455\n",
            "Epoch 2/5\n",
            "\u001b[1m573/573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 136ms/step - accuracy: 0.8159 - loss: 0.5958 - val_accuracy: 0.8322 - val_loss: 0.5673\n",
            "Epoch 3/5\n",
            "\u001b[1m573/573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 137ms/step - accuracy: 0.8469 - loss: 0.5025 - val_accuracy: 0.8382 - val_loss: 0.5452\n",
            "Epoch 4/5\n",
            "\u001b[1m573/573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 135ms/step - accuracy: 0.8640 - loss: 0.4460 - val_accuracy: 0.8485 - val_loss: 0.5101\n",
            "Epoch 5/5\n",
            "\u001b[1m573/573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 137ms/step - accuracy: 0.8740 - loss: 0.4064 - val_accuracy: 0.8530 - val_loss: 0.4966\n"
          ]
        }
      ],
      "source": [
        "history = rede_neural.fit(train_images_gray, y_treinamento, batch_size = 128,\n",
        "                epochs = 5, validation_data = (test_images_gray, y_teste))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
